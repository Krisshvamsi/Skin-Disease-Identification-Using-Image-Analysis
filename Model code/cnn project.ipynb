{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 769 images belonging to 5 classes.\n",
      "Found 142 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r\"D:\\project\\train\",target_size = (64,64),batch_size = 16)\n",
    "x_test = test_datagen.flow_from_directory(r\"D:\\project\\test\",target_size = (64,64),batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cold sores': 0, 'eczema': 1, 'hives': 2, 'melanoma': 3, 'psoriasis': 4}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(Convolution2D(32,(3,3),input_shape = (64,64,3),activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # input layer of cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 128 , init = \"uniform\",activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 100 , init = \"uniform\",activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 100 , init = \"uniform\",activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=5, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 5 , init = \"uniform\",activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\",optimizer = \"adam\",metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Rahul\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/200\n",
      "48/48 [==============================] - 17s 352ms/step - loss: 1.6144 - acc: 0.2070 - val_loss: 1.6086 - val_acc: 0.1875\n",
      "Epoch 2/200\n",
      "48/48 [==============================] - 10s 216ms/step - loss: 1.6041 - acc: 0.2188 - val_loss: 1.5513 - val_acc: 0.3413\n",
      "Epoch 3/200\n",
      "48/48 [==============================] - 11s 222ms/step - loss: 1.5521 - acc: 0.2849 - val_loss: 1.4766 - val_acc: 0.3016\n",
      "Epoch 4/200\n",
      "48/48 [==============================] - 10s 218ms/step - loss: 1.4621 - acc: 0.2931 - val_loss: 1.3818 - val_acc: 0.3175\n",
      "Epoch 5/200\n",
      "48/48 [==============================] - 10s 218ms/step - loss: 1.4357 - acc: 0.3009 - val_loss: 1.3678 - val_acc: 0.2937\n",
      "Epoch 6/200\n",
      "48/48 [==============================] - 10s 205ms/step - loss: 1.3943 - acc: 0.3139 - val_loss: 1.4289 - val_acc: 0.3016\n",
      "Epoch 7/200\n",
      "48/48 [==============================] - 10s 215ms/step - loss: 1.4548 - acc: 0.3244 - val_loss: 1.2559 - val_acc: 0.4524\n",
      "Epoch 8/200\n",
      "48/48 [==============================] - 11s 228ms/step - loss: 1.3653 - acc: 0.3513 - val_loss: 1.3298 - val_acc: 0.3254\n",
      "Epoch 9/200\n",
      "48/48 [==============================] - 10s 201ms/step - loss: 1.4032 - acc: 0.3113 - val_loss: 1.2477 - val_acc: 0.3730\n",
      "Epoch 10/200\n",
      "48/48 [==============================] - 10s 216ms/step - loss: 1.3544 - acc: 0.3296 - val_loss: 1.2344 - val_acc: 0.4297\n",
      "Epoch 11/200\n",
      "48/48 [==============================] - 11s 231ms/step - loss: 1.3045 - acc: 0.3595 - val_loss: 1.2272 - val_acc: 0.3889\n",
      "Epoch 12/200\n",
      "48/48 [==============================] - 10s 214ms/step - loss: 1.3445 - acc: 0.3244 - val_loss: 1.1934 - val_acc: 0.4444\n",
      "Epoch 13/200\n",
      "48/48 [==============================] - 10s 218ms/step - loss: 1.3054 - acc: 0.3865 - val_loss: 1.2288 - val_acc: 0.4206\n",
      "Epoch 14/200\n",
      "48/48 [==============================] - 11s 224ms/step - loss: 1.2411 - acc: 0.4194 - val_loss: 1.1150 - val_acc: 0.4841\n",
      "Epoch 15/200\n",
      "48/48 [==============================] - 11s 230ms/step - loss: 1.2447 - acc: 0.4216 - val_loss: 1.1000 - val_acc: 0.5397\n",
      "Epoch 16/200\n",
      "48/48 [==============================] - 12s 259ms/step - loss: 1.1867 - acc: 0.4829 - val_loss: 1.0752 - val_acc: 0.5397\n",
      "Epoch 17/200\n",
      "48/48 [==============================] - 19s 405ms/step - loss: 1.1608 - acc: 0.4676 - val_loss: 0.9878 - val_acc: 0.6270\n",
      "Epoch 18/200\n",
      "48/48 [==============================] - 13s 262ms/step - loss: 1.2231 - acc: 0.4181 - val_loss: 1.0616 - val_acc: 0.5000\n",
      "Epoch 19/200\n",
      "48/48 [==============================] - 13s 268ms/step - loss: 1.1506 - acc: 0.4846 - val_loss: 1.0817 - val_acc: 0.5391\n",
      "Epoch 20/200\n",
      "48/48 [==============================] - 12s 249ms/step - loss: 1.1994 - acc: 0.4637 - val_loss: 1.0003 - val_acc: 0.5397\n",
      "Epoch 21/200\n",
      "48/48 [==============================] - 13s 277ms/step - loss: 1.1476 - acc: 0.4859 - val_loss: 0.9824 - val_acc: 0.6111\n",
      "Epoch 22/200\n",
      "48/48 [==============================] - 12s 253ms/step - loss: 1.1302 - acc: 0.5063 - val_loss: 1.0691 - val_acc: 0.5159\n",
      "Epoch 23/200\n",
      "48/48 [==============================] - 13s 261ms/step - loss: 1.2269 - acc: 0.4689 - val_loss: 1.0498 - val_acc: 0.5238\n",
      "Epoch 24/200\n",
      "48/48 [==============================] - 11s 234ms/step - loss: 1.1302 - acc: 0.5076 - val_loss: 1.1113 - val_acc: 0.4841\n",
      "Epoch 25/200\n",
      "48/48 [==============================] - 12s 243ms/step - loss: 1.0695 - acc: 0.5597 - val_loss: 0.8793 - val_acc: 0.6508\n",
      "Epoch 26/200\n",
      "48/48 [==============================] - 13s 263ms/step - loss: 1.0264 - acc: 0.5272 - val_loss: 0.9907 - val_acc: 0.5714\n",
      "Epoch 27/200\n",
      "48/48 [==============================] - 13s 264ms/step - loss: 1.0393 - acc: 0.5132 - val_loss: 0.9866 - val_acc: 0.5714\n",
      "Epoch 28/200\n",
      "48/48 [==============================] - 11s 240ms/step - loss: 1.0020 - acc: 0.5636 - val_loss: 0.9953 - val_acc: 0.6094\n",
      "Epoch 29/200\n",
      "48/48 [==============================] - 13s 265ms/step - loss: 1.0393 - acc: 0.5679 - val_loss: 0.9627 - val_acc: 0.6032\n",
      "Epoch 30/200\n",
      "48/48 [==============================] - 13s 270ms/step - loss: 1.0838 - acc: 0.5458 - val_loss: 1.0066 - val_acc: 0.5238\n",
      "Epoch 31/200\n",
      "48/48 [==============================] - 12s 250ms/step - loss: 1.0367 - acc: 0.5484 - val_loss: 1.0806 - val_acc: 0.5476\n",
      "Epoch 32/200\n",
      "48/48 [==============================] - 11s 219ms/step - loss: 1.0363 - acc: 0.5666 - val_loss: 1.0461 - val_acc: 0.6508\n",
      "Epoch 33/200\n",
      "48/48 [==============================] - 11s 223ms/step - loss: 0.9883 - acc: 0.5975 - val_loss: 0.8961 - val_acc: 0.5794\n",
      "Epoch 34/200\n",
      "48/48 [==============================] - 14s 301ms/step - loss: 0.9778 - acc: 0.5636 - val_loss: 1.0424 - val_acc: 0.5794\n",
      "Epoch 35/200\n",
      "48/48 [==============================] - 14s 287ms/step - loss: 0.9217 - acc: 0.5949 - val_loss: 1.0234 - val_acc: 0.5952\n",
      "Epoch 36/200\n",
      "48/48 [==============================] - 17s 364ms/step - loss: 0.9038 - acc: 0.6327 - val_loss: 0.9018 - val_acc: 0.6429\n",
      "Epoch 37/200\n",
      "48/48 [==============================] - 14s 299ms/step - loss: 0.8641 - acc: 0.6248 - val_loss: 0.8427 - val_acc: 0.6172\n",
      "Epoch 38/200\n",
      "48/48 [==============================] - 16s 341ms/step - loss: 0.8428 - acc: 0.6522 - val_loss: 0.9706 - val_acc: 0.5714\n",
      "Epoch 39/200\n",
      "48/48 [==============================] - 16s 327ms/step - loss: 0.7990 - acc: 0.6483 - val_loss: 1.0241 - val_acc: 0.5952\n",
      "Epoch 40/200\n",
      "48/48 [==============================] - 15s 311ms/step - loss: 0.8434 - acc: 0.6470 - val_loss: 0.9074 - val_acc: 0.6190\n",
      "Epoch 41/200\n",
      "48/48 [==============================] - 16s 328ms/step - loss: 0.8861 - acc: 0.6279 - val_loss: 0.9697 - val_acc: 0.6746\n",
      "Epoch 42/200\n",
      "48/48 [==============================] - 15s 318ms/step - loss: 0.7674 - acc: 0.6861 - val_loss: 0.8919 - val_acc: 0.6746\n",
      "Epoch 43/200\n",
      "48/48 [==============================] - 16s 337ms/step - loss: 0.7956 - acc: 0.6691 - val_loss: 0.7599 - val_acc: 0.6984\n",
      "Epoch 44/200\n",
      "48/48 [==============================] - 17s 347ms/step - loss: 0.7618 - acc: 0.6704 - val_loss: 1.0272 - val_acc: 0.6032\n",
      "Epoch 45/200\n",
      "48/48 [==============================] - 14s 290ms/step - loss: 0.7163 - acc: 0.6956 - val_loss: 0.9678 - val_acc: 0.6349\n",
      "Epoch 46/200\n",
      "48/48 [==============================] - 16s 343ms/step - loss: 0.7581 - acc: 0.6991 - val_loss: 0.9363 - val_acc: 0.6406\n",
      "Epoch 47/200\n",
      "48/48 [==============================] - 16s 327ms/step - loss: 0.7007 - acc: 0.7073 - val_loss: 1.1744 - val_acc: 0.5317\n",
      "Epoch 48/200\n",
      "48/48 [==============================] - 15s 315ms/step - loss: 0.7324 - acc: 0.6991 - val_loss: 0.7412 - val_acc: 0.6905\n",
      "Epoch 49/200\n",
      "48/48 [==============================] - 14s 301ms/step - loss: 0.6227 - acc: 0.7447 - val_loss: 1.2263 - val_acc: 0.6111\n",
      "Epoch 50/200\n",
      "48/48 [==============================] - 15s 316ms/step - loss: 0.6888 - acc: 0.7318 - val_loss: 0.8411 - val_acc: 0.6825\n",
      "Epoch 51/200\n",
      "48/48 [==============================] - 16s 323ms/step - loss: 0.6990 - acc: 0.7238 - val_loss: 0.8497 - val_acc: 0.7302\n",
      "Epoch 52/200\n",
      "48/48 [==============================] - 15s 313ms/step - loss: 0.6156 - acc: 0.7369 - val_loss: 1.1294 - val_acc: 0.6746\n",
      "Epoch 53/200\n",
      "48/48 [==============================] - 16s 342ms/step - loss: 0.5522 - acc: 0.7877 - val_loss: 1.1520 - val_acc: 0.6508\n",
      "Epoch 54/200\n",
      "48/48 [==============================] - 15s 304ms/step - loss: 0.4959 - acc: 0.7994 - val_loss: 1.0121 - val_acc: 0.6984\n",
      "Epoch 55/200\n",
      "48/48 [==============================] - 16s 338ms/step - loss: 0.5316 - acc: 0.7773 - val_loss: 1.0582 - val_acc: 0.6719\n",
      "Epoch 56/200\n",
      "48/48 [==============================] - 15s 304ms/step - loss: 0.4825 - acc: 0.7994 - val_loss: 1.2515 - val_acc: 0.6905\n",
      "Epoch 57/200\n",
      "48/48 [==============================] - 16s 338ms/step - loss: 0.4970 - acc: 0.8111 - val_loss: 1.0377 - val_acc: 0.6508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "48/48 [==============================] - 15s 311ms/step - loss: 0.5860 - acc: 0.7620 - val_loss: 1.0045 - val_acc: 0.7302\n",
      "Epoch 59/200\n",
      "48/48 [==============================] - 14s 294ms/step - loss: 0.5569 - acc: 0.7851 - val_loss: 1.2251 - val_acc: 0.7143\n",
      "Epoch 60/200\n",
      "48/48 [==============================] - 14s 293ms/step - loss: 0.6080 - acc: 0.7564 - val_loss: 1.0786 - val_acc: 0.6508\n",
      "Epoch 61/200\n",
      "48/48 [==============================] - 15s 311ms/step - loss: 0.4683 - acc: 0.8111 - val_loss: 1.2130 - val_acc: 0.6984\n",
      "Epoch 62/200\n",
      "48/48 [==============================] - 15s 314ms/step - loss: 0.4711 - acc: 0.8202 - val_loss: 1.0126 - val_acc: 0.6984\n",
      "Epoch 63/200\n",
      "48/48 [==============================] - 13s 277ms/step - loss: 0.4155 - acc: 0.8359 - val_loss: 1.0171 - val_acc: 0.7063\n",
      "Epoch 64/200\n",
      "48/48 [==============================] - 15s 302ms/step - loss: 0.3917 - acc: 0.8489 - val_loss: 1.3352 - val_acc: 0.6953\n",
      "Epoch 65/200\n",
      "48/48 [==============================] - 15s 314ms/step - loss: 0.4783 - acc: 0.8215 - val_loss: 1.1251 - val_acc: 0.7302\n",
      "Epoch 66/200\n",
      "48/48 [==============================] - 13s 266ms/step - loss: 0.3366 - acc: 0.8710 - val_loss: 1.0807 - val_acc: 0.7063\n",
      "Epoch 67/200\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.4174 - acc: 0.8424 - val_loss: 1.2438 - val_acc: 0.6270\n",
      "Epoch 68/200\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.3529 - acc: 0.8619 - val_loss: 1.0952 - val_acc: 0.7143\n",
      "Epoch 69/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.4950 - acc: 0.8059 - val_loss: 0.9679 - val_acc: 0.7302\n",
      "Epoch 70/200\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.3863 - acc: 0.8580 - val_loss: 1.4811 - val_acc: 0.7222\n",
      "Epoch 71/200\n",
      "48/48 [==============================] - 13s 278ms/step - loss: 0.3479 - acc: 0.8658 - val_loss: 1.0643 - val_acc: 0.7540\n",
      "Epoch 72/200\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.3428 - acc: 0.8749 - val_loss: 1.3426 - val_acc: 0.6905\n",
      "Epoch 73/200\n",
      "48/48 [==============================] - 20s 415ms/step - loss: 0.3046 - acc: 0.9062 - val_loss: 1.3086 - val_acc: 0.7188\n",
      "Epoch 74/200\n",
      "48/48 [==============================] - 11s 235ms/step - loss: 0.2956 - acc: 0.8984 - val_loss: 1.1625 - val_acc: 0.7143\n",
      "Epoch 75/200\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.2728 - acc: 0.9010 - val_loss: 1.4834 - val_acc: 0.7381\n",
      "Epoch 76/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.3460 - acc: 0.8697 - val_loss: 1.6369 - val_acc: 0.7143\n",
      "Epoch 77/200\n",
      "48/48 [==============================] - 12s 241ms/step - loss: 0.3413 - acc: 0.8815 - val_loss: 1.0166 - val_acc: 0.7381\n",
      "Epoch 78/200\n",
      "48/48 [==============================] - 10s 206ms/step - loss: 0.3263 - acc: 0.8958 - val_loss: 1.5590 - val_acc: 0.6825\n",
      "Epoch 79/200\n",
      "48/48 [==============================] - 11s 234ms/step - loss: 0.2338 - acc: 0.9179 - val_loss: 1.2384 - val_acc: 0.7540\n",
      "Epoch 80/200\n",
      "48/48 [==============================] - 12s 247ms/step - loss: 0.2989 - acc: 0.9062 - val_loss: 1.0927 - val_acc: 0.7460\n",
      "Epoch 81/200\n",
      "48/48 [==============================] - 11s 231ms/step - loss: 0.2616 - acc: 0.9140 - val_loss: 1.4119 - val_acc: 0.6905\n",
      "Epoch 82/200\n",
      "48/48 [==============================] - 10s 213ms/step - loss: 0.2454 - acc: 0.9166 - val_loss: 1.0973 - val_acc: 0.7500\n",
      "Epoch 83/200\n",
      "48/48 [==============================] - 11s 227ms/step - loss: 0.2385 - acc: 0.9244 - val_loss: 1.4925 - val_acc: 0.7302\n",
      "Epoch 84/200\n",
      "48/48 [==============================] - 11s 229ms/step - loss: 0.2129 - acc: 0.9401 - val_loss: 1.3228 - val_acc: 0.7222\n",
      "Epoch 85/200\n",
      "48/48 [==============================] - 10s 218ms/step - loss: 0.2260 - acc: 0.9349 - val_loss: 1.2462 - val_acc: 0.7381\n",
      "Epoch 86/200\n",
      "48/48 [==============================] - 11s 224ms/step - loss: 0.2749 - acc: 0.9088 - val_loss: 1.2369 - val_acc: 0.7937\n",
      "Epoch 87/200\n",
      "48/48 [==============================] - 10s 217ms/step - loss: 0.1926 - acc: 0.9310 - val_loss: 1.7788 - val_acc: 0.6905\n",
      "Epoch 88/200\n",
      "48/48 [==============================] - 10s 208ms/step - loss: 0.1662 - acc: 0.9414 - val_loss: 1.4136 - val_acc: 0.7460\n",
      "Epoch 89/200\n",
      "48/48 [==============================] - 11s 221ms/step - loss: 0.1333 - acc: 0.9531 - val_loss: 1.5096 - val_acc: 0.7460\n",
      "Epoch 90/200\n",
      "48/48 [==============================] - 11s 227ms/step - loss: 0.1775 - acc: 0.9427 - val_loss: 1.6659 - val_acc: 0.7540\n",
      "Epoch 91/200\n",
      "48/48 [==============================] - 11s 226ms/step - loss: 0.1880 - acc: 0.9401 - val_loss: 1.4239 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "48/48 [==============================] - 10s 215ms/step - loss: 0.1147 - acc: 0.9583 - val_loss: 1.8289 - val_acc: 0.7222\n",
      "Epoch 93/200\n",
      "48/48 [==============================] - 12s 245ms/step - loss: 0.2020 - acc: 0.9336 - val_loss: 1.6391 - val_acc: 0.7302\n",
      "Epoch 94/200\n",
      "48/48 [==============================] - 11s 233ms/step - loss: 0.2219 - acc: 0.9166 - val_loss: 1.4663 - val_acc: 0.7063\n",
      "Epoch 95/200\n",
      "48/48 [==============================] - 11s 229ms/step - loss: 0.1388 - acc: 0.9505 - val_loss: 1.5129 - val_acc: 0.7619\n",
      "Epoch 96/200\n",
      "48/48 [==============================] - 11s 219ms/step - loss: 0.1258 - acc: 0.9596 - val_loss: 1.6788 - val_acc: 0.7143\n",
      "Epoch 97/200\n",
      "48/48 [==============================] - 12s 255ms/step - loss: 0.1091 - acc: 0.9596 - val_loss: 1.3089 - val_acc: 0.7619\n",
      "Epoch 98/200\n",
      "48/48 [==============================] - 13s 279ms/step - loss: 0.1271 - acc: 0.9596 - val_loss: 1.3021 - val_acc: 0.7460\n",
      "Epoch 99/200\n",
      "48/48 [==============================] - 12s 249ms/step - loss: 0.1452 - acc: 0.9466 - val_loss: 1.6156 - val_acc: 0.7302\n",
      "Epoch 100/200\n",
      "48/48 [==============================] - 14s 285ms/step - loss: 0.1566 - acc: 0.9518 - val_loss: 1.5150 - val_acc: 0.7266\n",
      "Epoch 101/200\n",
      "48/48 [==============================] - 22s 465ms/step - loss: 0.1785 - acc: 0.9557 - val_loss: 1.4403 - val_acc: 0.7302\n",
      "Epoch 102/200\n",
      "48/48 [==============================] - 19s 389ms/step - loss: 0.1459 - acc: 0.9479 - val_loss: 1.6668 - val_acc: 0.7143\n",
      "Epoch 103/200\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.6675 - acc: 0.8128 - val_loss: 1.2000 - val_acc: 0.7222\n",
      "Epoch 104/200\n",
      "48/48 [==============================] - 16s 332ms/step - loss: 0.2657 - acc: 0.9062 - val_loss: 0.9974 - val_acc: 0.7698\n",
      "Epoch 105/200\n",
      "48/48 [==============================] - 15s 318ms/step - loss: 0.1933 - acc: 0.9375 - val_loss: 0.9196 - val_acc: 0.7857\n",
      "Epoch 106/200\n",
      "48/48 [==============================] - 16s 329ms/step - loss: 0.2785 - acc: 0.8893 - val_loss: 1.3570 - val_acc: 0.7460\n",
      "Epoch 107/200\n",
      "48/48 [==============================] - 16s 331ms/step - loss: 0.2628 - acc: 0.9157 - val_loss: 1.1224 - val_acc: 0.7778\n",
      "Epoch 108/200\n",
      "48/48 [==============================] - 15s 309ms/step - loss: 0.1479 - acc: 0.9427 - val_loss: 1.2521 - val_acc: 0.7619\n",
      "Epoch 109/200\n",
      "48/48 [==============================] - 17s 359ms/step - loss: 0.0806 - acc: 0.9805 - val_loss: 1.4798 - val_acc: 0.7734\n",
      "Epoch 110/200\n",
      "48/48 [==============================] - 16s 331ms/step - loss: 0.1130 - acc: 0.9648 - val_loss: 1.3476 - val_acc: 0.7540\n",
      "Epoch 111/200\n",
      "48/48 [==============================] - 12s 259ms/step - loss: 0.0912 - acc: 0.9726 - val_loss: 1.9125 - val_acc: 0.7302\n",
      "Epoch 112/200\n",
      "48/48 [==============================] - 18s 372ms/step - loss: 0.0984 - acc: 0.9713 - val_loss: 1.3864 - val_acc: 0.7698\n",
      "Epoch 113/200\n",
      "48/48 [==============================] - 15s 313ms/step - loss: 0.1092 - acc: 0.9622 - val_loss: 1.6607 - val_acc: 0.7381\n",
      "Epoch 114/200\n",
      "48/48 [==============================] - 16s 336ms/step - loss: 0.0671 - acc: 0.9739 - val_loss: 1.5567 - val_acc: 0.7381\n",
      "Epoch 115/200\n",
      "48/48 [==============================] - 17s 354ms/step - loss: 0.0571 - acc: 0.9831 - val_loss: 1.2968 - val_acc: 0.7778\n",
      "Epoch 116/200\n",
      "48/48 [==============================] - 12s 249ms/step - loss: 0.0434 - acc: 0.9870 - val_loss: 1.7898 - val_acc: 0.6905\n",
      "Epoch 117/200\n",
      "48/48 [==============================] - 20s 412ms/step - loss: 0.0818 - acc: 0.9739 - val_loss: 1.7074 - val_acc: 0.7698\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 14s 289ms/step - loss: 0.0631 - acc: 0.9857 - val_loss: 1.8332 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "48/48 [==============================] - 16s 341ms/step - loss: 0.0553 - acc: 0.9818 - val_loss: 1.5621 - val_acc: 0.7460\n",
      "Epoch 120/200\n",
      "48/48 [==============================] - 14s 288ms/step - loss: 0.1080 - acc: 0.9674 - val_loss: 1.5381 - val_acc: 0.8016\n",
      "Epoch 121/200\n",
      "48/48 [==============================] - 14s 291ms/step - loss: 0.0443 - acc: 0.9870 - val_loss: 1.9079 - val_acc: 0.7460\n",
      "Epoch 122/200\n",
      "48/48 [==============================] - 19s 386ms/step - loss: 0.0316 - acc: 0.9935 - val_loss: 1.9257 - val_acc: 0.7619\n",
      "Epoch 123/200\n",
      "48/48 [==============================] - 14s 301ms/step - loss: 0.0572 - acc: 0.9857 - val_loss: 1.2138 - val_acc: 0.7619\n",
      "Epoch 124/200\n",
      "48/48 [==============================] - 15s 322ms/step - loss: 0.0721 - acc: 0.9818 - val_loss: 1.4261 - val_acc: 0.8175\n",
      "Epoch 125/200\n",
      "48/48 [==============================] - 14s 293ms/step - loss: 0.0488 - acc: 0.9870 - val_loss: 2.1588 - val_acc: 0.7222\n",
      "Epoch 126/200\n",
      "48/48 [==============================] - 17s 352ms/step - loss: 0.1181 - acc: 0.9687 - val_loss: 1.7868 - val_acc: 0.7540\n",
      "Epoch 127/200\n",
      "48/48 [==============================] - 14s 302ms/step - loss: 0.1796 - acc: 0.9427 - val_loss: 1.4577 - val_acc: 0.7656\n",
      "Epoch 128/200\n",
      "48/48 [==============================] - 15s 303ms/step - loss: 0.1046 - acc: 0.9648 - val_loss: 1.4720 - val_acc: 0.7460\n",
      "Epoch 129/200\n",
      "48/48 [==============================] - 16s 340ms/step - loss: 0.0691 - acc: 0.9766 - val_loss: 1.7177 - val_acc: 0.7857\n",
      "Epoch 130/200\n",
      "48/48 [==============================] - 13s 277ms/step - loss: 0.0684 - acc: 0.9831 - val_loss: 1.3972 - val_acc: 0.7937\n",
      "Epoch 131/200\n",
      "48/48 [==============================] - 16s 333ms/step - loss: 0.0434 - acc: 0.9857 - val_loss: 2.1626 - val_acc: 0.7460\n",
      "Epoch 132/200\n",
      "48/48 [==============================] - 17s 351ms/step - loss: 0.1356 - acc: 0.9531 - val_loss: 1.4001 - val_acc: 0.7857\n",
      "Epoch 133/200\n",
      "48/48 [==============================] - 15s 302ms/step - loss: 0.0491 - acc: 0.9805 - val_loss: 1.2769 - val_acc: 0.7778\n",
      "Epoch 134/200\n",
      "48/48 [==============================] - 18s 384ms/step - loss: 0.0551 - acc: 0.9857 - val_loss: 1.8269 - val_acc: 0.7143\n",
      "Epoch 135/200\n",
      "48/48 [==============================] - 15s 314ms/step - loss: 0.0778 - acc: 0.9779 - val_loss: 1.3893 - val_acc: 0.7540\n",
      "Epoch 136/200\n",
      "48/48 [==============================] - 15s 311ms/step - loss: 0.0632 - acc: 0.9779 - val_loss: 1.5659 - val_acc: 0.7656\n",
      "Epoch 137/200\n",
      "48/48 [==============================] - 16s 337ms/step - loss: 0.0954 - acc: 0.9753 - val_loss: 1.9041 - val_acc: 0.7381\n",
      "Epoch 138/200\n",
      "48/48 [==============================] - 13s 275ms/step - loss: 0.0802 - acc: 0.9726 - val_loss: 1.9322 - val_acc: 0.7619\n",
      "Epoch 139/200\n",
      "48/48 [==============================] - 13s 271ms/step - loss: 0.0508 - acc: 0.9883 - val_loss: 1.8344 - val_acc: 0.7460\n",
      "Epoch 140/200\n",
      "48/48 [==============================] - 15s 316ms/step - loss: 0.1309 - acc: 0.9661 - val_loss: 1.4363 - val_acc: 0.7460\n",
      "Epoch 141/200\n",
      "48/48 [==============================] - 14s 300ms/step - loss: 0.0878 - acc: 0.9726 - val_loss: 1.1168 - val_acc: 0.8095\n",
      "Epoch 142/200\n",
      "48/48 [==============================] - 15s 304ms/step - loss: 0.0636 - acc: 0.9766 - val_loss: 1.9126 - val_acc: 0.7302\n",
      "Epoch 143/200\n",
      "48/48 [==============================] - 14s 292ms/step - loss: 0.0246 - acc: 0.9948 - val_loss: 1.9639 - val_acc: 0.6984\n",
      "Epoch 144/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.0338 - acc: 0.9896 - val_loss: 1.8610 - val_acc: 0.7381\n",
      "Epoch 145/200\n",
      "48/48 [==============================] - 15s 310ms/step - loss: 0.0443 - acc: 0.9857 - val_loss: 2.0287 - val_acc: 0.7578\n",
      "Epoch 146/200\n",
      "48/48 [==============================] - 15s 303ms/step - loss: 0.0825 - acc: 0.9753 - val_loss: 1.5828 - val_acc: 0.7937\n",
      "Epoch 147/200\n",
      "48/48 [==============================] - 15s 304ms/step - loss: 0.0363 - acc: 0.9883 - val_loss: 1.8562 - val_acc: 0.7540\n",
      "Epoch 148/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.0464 - acc: 0.9844 - val_loss: 1.7736 - val_acc: 0.7857\n",
      "Epoch 149/200\n",
      "48/48 [==============================] - 15s 303ms/step - loss: 0.0669 - acc: 0.9753 - val_loss: 1.9756 - val_acc: 0.7937\n",
      "Epoch 150/200\n",
      "48/48 [==============================] - 15s 305ms/step - loss: 0.0682 - acc: 0.9753 - val_loss: 2.1583 - val_acc: 0.7222\n",
      "Epoch 151/200\n",
      "48/48 [==============================] - 15s 303ms/step - loss: 0.0901 - acc: 0.9713 - val_loss: 1.5261 - val_acc: 0.8016\n",
      "Epoch 152/200\n",
      "48/48 [==============================] - 14s 297ms/step - loss: 0.0378 - acc: 0.9883 - val_loss: 1.8767 - val_acc: 0.7540\n",
      "Epoch 153/200\n",
      "48/48 [==============================] - 15s 316ms/step - loss: 0.0939 - acc: 0.9726 - val_loss: 1.8727 - val_acc: 0.8016\n",
      "Epoch 154/200\n",
      "48/48 [==============================] - 14s 288ms/step - loss: 0.0949 - acc: 0.9753 - val_loss: 1.7653 - val_acc: 0.7578\n",
      "Epoch 155/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.1238 - acc: 0.9713 - val_loss: 1.9481 - val_acc: 0.7222\n",
      "Epoch 156/200\n",
      "48/48 [==============================] - 15s 319ms/step - loss: 0.0314 - acc: 0.9896 - val_loss: 2.1145 - val_acc: 0.7778\n",
      "Epoch 157/200\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.0145 - acc: 0.9987 - val_loss: 1.2488 - val_acc: 0.8016\n",
      "Epoch 158/200\n",
      "48/48 [==============================] - 12s 250ms/step - loss: 0.0191 - acc: 0.9922 - val_loss: 1.9457 - val_acc: 0.7619\n",
      "Epoch 159/200\n",
      "48/48 [==============================] - 16s 331ms/step - loss: 0.0325 - acc: 0.9883 - val_loss: 2.0869 - val_acc: 0.7619\n",
      "Epoch 160/200\n",
      "48/48 [==============================] - 15s 322ms/step - loss: 0.0084 - acc: 0.9987 - val_loss: 1.4727 - val_acc: 0.7937\n",
      "Epoch 161/200\n",
      "48/48 [==============================] - 14s 291ms/step - loss: 0.0313 - acc: 0.9896 - val_loss: 1.8384 - val_acc: 0.7460\n",
      "Epoch 162/200\n",
      "48/48 [==============================] - 15s 306ms/step - loss: 0.0488 - acc: 0.9909 - val_loss: 1.6823 - val_acc: 0.7778\n",
      "Epoch 163/200\n",
      "48/48 [==============================] - 14s 290ms/step - loss: 0.0347 - acc: 0.9896 - val_loss: 1.7942 - val_acc: 0.8047\n",
      "Epoch 164/200\n",
      "48/48 [==============================] - 16s 325ms/step - loss: 0.0348 - acc: 0.9883 - val_loss: 1.8052 - val_acc: 0.7540\n",
      "Epoch 165/200\n",
      "48/48 [==============================] - 13s 281ms/step - loss: 0.0297 - acc: 0.9935 - val_loss: 1.6641 - val_acc: 0.7778\n",
      "Epoch 166/200\n",
      "48/48 [==============================] - 16s 329ms/step - loss: 0.1072 - acc: 0.9687 - val_loss: 1.3708 - val_acc: 0.7937\n",
      "Epoch 167/200\n",
      "48/48 [==============================] - 14s 295ms/step - loss: 0.1065 - acc: 0.9635 - val_loss: 1.9123 - val_acc: 0.7698\n",
      "Epoch 168/200\n",
      "48/48 [==============================] - 14s 291ms/step - loss: 0.0520 - acc: 0.9844 - val_loss: 1.6788 - val_acc: 0.7698\n",
      "Epoch 169/200\n",
      "48/48 [==============================] - 15s 304ms/step - loss: 0.0327 - acc: 0.9922 - val_loss: 1.3233 - val_acc: 0.8175\n",
      "Epoch 170/200\n",
      "48/48 [==============================] - 14s 296ms/step - loss: 0.0515 - acc: 0.9857 - val_loss: 1.7030 - val_acc: 0.7619\n",
      "Epoch 171/200\n",
      "48/48 [==============================] - 15s 305ms/step - loss: 0.1223 - acc: 0.9557 - val_loss: 1.7915 - val_acc: 0.7381\n",
      "Epoch 172/200\n",
      "48/48 [==============================] - 14s 293ms/step - loss: 0.0975 - acc: 0.9674 - val_loss: 1.8189 - val_acc: 0.7578\n",
      "Epoch 173/200\n",
      "48/48 [==============================] - 16s 326ms/step - loss: 0.0520 - acc: 0.9883 - val_loss: 1.6150 - val_acc: 0.7857\n",
      "Epoch 174/200\n",
      "48/48 [==============================] - 14s 296ms/step - loss: 0.0512 - acc: 0.9831 - val_loss: 1.4978 - val_acc: 0.7937\n",
      "Epoch 175/200\n",
      "48/48 [==============================] - 13s 277ms/step - loss: 0.0393 - acc: 0.9857 - val_loss: 1.6585 - val_acc: 0.7778\n",
      "Epoch 176/200\n",
      "48/48 [==============================] - 17s 348ms/step - loss: 0.0180 - acc: 0.9935 - val_loss: 1.6364 - val_acc: 0.7381\n",
      "Epoch 177/200\n",
      "48/48 [==============================] - 13s 270ms/step - loss: 0.0311 - acc: 0.9896 - val_loss: 1.8893 - val_acc: 0.7381\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 15s 306ms/step - loss: 0.0371 - acc: 0.9857 - val_loss: 1.6049 - val_acc: 0.8254\n",
      "Epoch 179/200\n",
      "48/48 [==============================] - 16s 343ms/step - loss: 0.0364 - acc: 0.9883 - val_loss: 1.7447 - val_acc: 0.7619\n",
      "Epoch 180/200\n",
      "48/48 [==============================] - 12s 242ms/step - loss: 0.0441 - acc: 0.9896 - val_loss: 1.6340 - val_acc: 0.8016\n",
      "Epoch 181/200\n",
      "48/48 [==============================] - 15s 313ms/step - loss: 0.0248 - acc: 0.9896 - val_loss: 1.7896 - val_acc: 0.7734\n",
      "Epoch 182/200\n",
      "48/48 [==============================] - 16s 343ms/step - loss: 0.0566 - acc: 0.9857 - val_loss: 1.9581 - val_acc: 0.7302\n",
      "Epoch 183/200\n",
      "48/48 [==============================] - 15s 310ms/step - loss: 0.0389 - acc: 0.9896 - val_loss: 1.7617 - val_acc: 0.8016\n",
      "Epoch 184/200\n",
      "48/48 [==============================] - 15s 313ms/step - loss: 0.0446 - acc: 0.9805 - val_loss: 1.3812 - val_acc: 0.7937\n",
      "Epoch 185/200\n",
      "48/48 [==============================] - 16s 325ms/step - loss: 0.0300 - acc: 0.9896 - val_loss: 1.8645 - val_acc: 0.7857\n",
      "Epoch 186/200\n",
      "48/48 [==============================] - 15s 307ms/step - loss: 0.0359 - acc: 0.9831 - val_loss: 2.0410 - val_acc: 0.7619\n",
      "Epoch 187/200\n",
      "48/48 [==============================] - 17s 346ms/step - loss: 0.0386 - acc: 0.9857 - val_loss: 1.6452 - val_acc: 0.7937\n",
      "Epoch 188/200\n",
      "48/48 [==============================] - 14s 294ms/step - loss: 0.0511 - acc: 0.9883 - val_loss: 1.7257 - val_acc: 0.7460\n",
      "Epoch 189/200\n",
      "48/48 [==============================] - 15s 320ms/step - loss: 0.0598 - acc: 0.9857 - val_loss: 1.6834 - val_acc: 0.7778\n",
      "Epoch 190/200\n",
      "48/48 [==============================] - 15s 308ms/step - loss: 0.0443 - acc: 0.9818 - val_loss: 1.4553 - val_acc: 0.7891\n",
      "Epoch 191/200\n",
      "48/48 [==============================] - 15s 314ms/step - loss: 0.0845 - acc: 0.9739 - val_loss: 2.3011 - val_acc: 0.6825\n",
      "Epoch 192/200\n",
      "48/48 [==============================] - 14s 300ms/step - loss: 0.0413 - acc: 0.9805 - val_loss: 1.5365 - val_acc: 0.7778\n",
      "Epoch 193/200\n",
      "48/48 [==============================] - 14s 290ms/step - loss: 0.0683 - acc: 0.9805 - val_loss: 1.8855 - val_acc: 0.7698\n",
      "Epoch 194/200\n",
      "48/48 [==============================] - 11s 225ms/step - loss: 0.0762 - acc: 0.9713 - val_loss: 1.5808 - val_acc: 0.7619\n",
      "Epoch 195/200\n",
      "48/48 [==============================] - 11s 222ms/step - loss: 0.0630 - acc: 0.9805 - val_loss: 2.0054 - val_acc: 0.7222\n",
      "Epoch 196/200\n",
      "48/48 [==============================] - 11s 224ms/step - loss: 0.0856 - acc: 0.9713 - val_loss: 1.6180 - val_acc: 0.7698\n",
      "Epoch 197/200\n",
      "48/48 [==============================] - 11s 220ms/step - loss: 0.0249 - acc: 0.9948 - val_loss: 1.7051 - val_acc: 0.7619\n",
      "Epoch 198/200\n",
      "48/48 [==============================] - 10s 215ms/step - loss: 0.0423 - acc: 0.9870 - val_loss: 1.6425 - val_acc: 0.7778\n",
      "Epoch 199/200\n",
      "48/48 [==============================] - 11s 222ms/step - loss: 0.0354 - acc: 0.9883 - val_loss: 1.8471 - val_acc: 0.7578\n",
      "Epoch 200/200\n",
      "48/48 [==============================] - 11s 221ms/step - loss: 0.0348 - acc: 0.9922 - val_loss: 1.8557 - val_acc: 0.7381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a41843c48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(x_train, steps_per_epoch = 48,validation_data = x_test,validation_steps = 8 ,epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"skin_proj.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
